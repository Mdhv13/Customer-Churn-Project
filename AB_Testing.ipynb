{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc51154-8001-4b51-81be-079ae1aeb794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import PipelineModel\n",
    "from scipy.stats import ttest_ind\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efcdf030-3f37-442b-ad78-ec7b6b817cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ABTesting\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42b9743f-380b-474b-9611-b6becc188414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data and trained models\n",
    "test_df = spark.read.parquet(\"test_data.parquet\")\n",
    "model_A = PipelineModel.load(\"model_A\")\n",
    "model_B = PipelineModel.load(\"model_B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4e33db-090c-46fc-b7cd-2ec98c1b57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with both models\n",
    "predictions_A = model_A.transform(test_df)\n",
    "predictions_B = model_B.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4325945a-5126-44d8-9e3a-7c59c2407bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A (Logistic Regression) F1 Score: 0.7274\n",
      "Model B (Gradient Boosted Tree) F1 Score: 0.7314\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"churn\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_A = evaluator.evaluate(predictions_A)\n",
    "f1_B = evaluator.evaluate(predictions_B)\n",
    "\n",
    "print(f\"Model A (Logistic Regression) F1 Score: {f1_A:.4f}\")\n",
    "print(f\"Model B (Gradient Boosted Tree) F1 Score: {f1_B:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8c3498f-60b7-4d30-8211-9d1d9fb50fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a statistical t-test on the predictions\n",
    "# We need to collect the predictions to do this with SciPy\n",
    "df_A_pandas = predictions_A.select(\"churn\", \"prediction\").toPandas()\n",
    "df_B_pandas = predictions_B.select(\"churn\", \"prediction\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eae1309-368e-4c5b-a2fc-1c6c09bd257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate whether the prediction was correct (1) or not (0)\n",
    "df_A_pandas['is_correct'] = (df_A_pandas['churn'] == df_A_pandas['prediction']).astype(int)\n",
    "df_B_pandas['is_correct'] = (df_B_pandas['churn'] == df_B_pandas['prediction']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3a13986-e66d-47e2-a6ee-167acbc7b56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results of the A/B Test (t-test):\n",
      "t-statistic: -0.4864\n",
      "p-value: 0.6267\n"
     ]
    }
   ],
   "source": [
    "# Perform a t-test on the correctness scores\n",
    "t_stat, p_value = ttest_ind(df_A_pandas['is_correct'], df_B_pandas['is_correct'])\n",
    "\n",
    "print(f\"\\nResults of the A/B Test (t-test):\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac9981e8-64d7-4b56-9e10-4349a721759e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conclusion: The difference in performance is not statistically significant.\n",
      "There is no clear winner between the two models.\n"
     ]
    }
   ],
   "source": [
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"\\nConclusion: The difference in performance is statistically significant.\")\n",
    "    if f1_B > f1_A:\n",
    "        print(\"Model B (Gradient Boosted Tree) is the winner!\")\n",
    "    else:\n",
    "        print(\"Model A (Logistic Regression) is the winner!\")\n",
    "else:\n",
    "    print(\"\\nConclusion: The difference in performance is not statistically significant.\")\n",
    "    print(\"There is no clear winner between the two models.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
