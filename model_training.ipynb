{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec111844-a66c-4aa1-b1d9-c2882e8f22f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-4.0.0.tar.gz (434.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.9 (from pyspark)\n",
      "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-4.0.0-py2.py3-none-any.whl size=434741239 sha256=754f5908cb161df7900b66812ac1d7610b2a065c0ea64e910f6ffd14237752c4\n",
      "  Stored in directory: /Users/madhav/Library/Caches/pip/wheels/2d/77/9b/12660be70f7f447940a0caede37ae208b2e0d1c8487dce52a6\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.9 pyspark-4.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd58f0db-0408-4210-b2c8-41ddeab78fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7150229-d6a8-4416-aba4-38cc0fe91768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/03 09:24:33 WARN Utils: Your hostname, Madhavs-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.29.26 instead (on interface en0)\n",
      "25/09/03 09:24:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/03 09:24:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------------------+----------------------+--------------+------------------+-----+\n",
      "|customer_id|tenure|   monthly_charges|customer_service_calls| contract_type|       total_spend|churn|\n",
      "+-----------+------+------------------+----------------------+--------------+------------------+-----+\n",
      "|      49360|    28| 83.10578497022318|                     5|      Two year|2421.3282252871136|    0|\n",
      "|     579739|     4|115.51495708804909|                     2|Month-to-month| 544.0921915409324|    1|\n",
      "|     176055|    28| 82.07744951126622|                     4|Month-to-month| 2564.718560335923|    1|\n",
      "|     644304|    71|138.45030504249746|                     6|      Two year| 9198.776074232932|    1|\n",
      "|     382184|    95| 80.25744812322104|                     8|      Two year| 8078.927108991743|    0|\n",
      "+-----------+------+------------------+----------------------+--------------+------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"ChurnPrediction\").getOrCreate()\n",
    "\n",
    "# Load the data\n",
    "df = spark.read.csv(\"customer_data.csv\", header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4af0639-d700-4632-b2ac-feab9c685069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for modeling\n",
    "# 1. Handle categorical variables\n",
    "categorical_cols = ['contract_type']\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\") for col in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=f\"{col}_indexed\", outputCol=f\"{col}_vec\") for col in categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa963ff5-9971-4ac9-bc3f-459b43a2d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Combine all features into a single vector\n",
    "feature_cols = ['tenure', 'monthly_charges', 'customer_service_calls', 'total_spend']\n",
    "feature_cols_final = feature_cols + [f\"{col}_vec\" for col in categorical_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols_final, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f7b946-629a-42f7-90f5-21c9d84ab9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create the two models\n",
    "lr = LogisticRegression(labelCol=\"churn\", featuresCol=\"features\") # Model A (Control)\n",
    "gbt = GBTClassifier(labelCol=\"churn\", featuresCol=\"features\", maxIter=10) # Model B (Challenger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a31c310-7512-45da-8329-24e1b8b211dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a single pipeline for each model\n",
    "pipeline_lr = Pipeline(stages=indexers + encoders + [assembler, lr])\n",
    "pipeline_gbt = Pipeline(stages=indexers + encoders + [assembler, gbt])\n",
    "\n",
    "# Split data for training and testing\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e43a21e-8e06-48f5-8346-a234a2378724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models\n",
    "model_A = pipeline_lr.fit(train_df)\n",
    "model_B = pipeline_gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1f317b0-f301-4f50-844a-ddbc1cb74e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "model_A.write().overwrite().save(\"model_A\")\n",
    "model_B.write().overwrite().save(\"model_B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "066a2fa7-75f6-499f-a7bb-f443713db94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models trained and saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the test data to evaluate later\n",
    "test_df.write.parquet(\"test_data.parquet\", mode=\"overwrite\")\n",
    "\n",
    "print(\"Models trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd341a7d-5740-4da6-8691-6a54f398a98d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
